# LibreChat Configuration for GCP Vertex AI Model Garden + Judge0 Code Interpreter
# Judge0 integration is built-in to Code Interpreter (no MCP needed)

version: 1.3.1

cache: true

# Human-friendly model names for better UX
modelSpecs:
  list:
    # OpenAI Models
    - name: "gpt-4o"
      label: "GPT-4o"
      description: "Most capable multimodal GPT-4 - fast and intelligent"
      group: "openAI"
      preset:
        endpoint: "openAI"
        model: "gpt-4o"

    - name: "gpt-4o-mini"
      label: "GPT-4o Mini"
      description: "Smaller, faster GPT-4o - excellent value"
      group: "openAI"
      preset:
        endpoint: "openAI"
        model: "gpt-4o-mini"

    - name: "o1"
      label: "o1"
      description: "Advanced reasoning model - best for complex problems"
      group: "openAI"
      preset:
        endpoint: "openAI"
        model: "o1"

    - name: "o1-mini"
      label: "o1 Mini"
      description: "Faster reasoning model - good for coding and STEM"
      group: "openAI"
      preset:
        endpoint: "openAI"
        model: "o1-mini"

    - name: "o1-pro"
      label: "o1 Pro"
      description: "Most advanced reasoning - for hardest problems"
      group: "openAI"
      preset:
        endpoint: "openAI"
        model: "o1-pro"

    - name: "gpt-4-turbo"
      label: "GPT-4 Turbo"
      description: "Previous flagship - 128k context"
      group: "openAI"
      preset:
        endpoint: "openAI"
        model: "gpt-4-turbo"

    - name: "gpt-3.5-turbo"
      label: "GPT-3.5 Turbo"
      description: "Fast and affordable - great for simple tasks"
      group: "openAI"
      preset:
        endpoint: "openAI"
        model: "gpt-3.5-turbo"

    # Google Gemini Models
    - name: "gemini-3-pro-preview"
      label: "Gemini 3 Pro Preview"
      description: "Google's flagship model with 1M context - state-of-the-art multimodal reasoning"
      group: "google"
      preset:
        endpoint: "google"
        model: "gemini-3-pro-preview"

    - name: "gemini-2.5-pro"
      label: "Gemini 2.5 Pro"
      description: "Most capable Gemini 2.5 model - excellent for complex tasks"
      group: "google"
      preset:
        endpoint: "google"
        model: "gemini-2.5-pro"

    - name: "gemini-2.5-flash"
      label: "Gemini 2.5 Flash"
      description: "Fast and efficient - balanced performance"
      group: "google"
      preset:
        endpoint: "google"
        model: "gemini-2.5-flash"

    - name: "gemini-2.5-flash-lite"
      label: "Gemini 2.5 Flash Lite"
      description: "Lightweight and fast - great for quick tasks"
      group: "google"
      preset:
        endpoint: "google"
        model: "gemini-2.5-flash-lite"

    - name: "gemini-2.0-flash"
      label: "Gemini 2.0 Flash"
      description: "Previous generation fast model"
      group: "google"
      preset:
        endpoint: "google"
        model: "gemini-2.0-flash"

    - name: "gemini-2.0-flash-lite"
      label: "Gemini 2.0 Flash Lite"
      description: "Previous generation lightweight model"
      group: "google"
      preset:
        endpoint: "google"
        model: "gemini-2.0-flash-lite"

    # Vertex AI Models
    - name: "deepseek-r1"
      label: "DeepSeek R1"
      description: "Advanced reasoning model - excels at complex problem solving"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "deepseek-r1"

    - name: "deepseek-v3"
      label: "DeepSeek V3"
      description: "General purpose model - balanced performance"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "deepseek-v3"

    - name: "qwen3-thinking"
      label: "Qwen3 Next 80B Thinking"
      description: "Thinking mode - shows reasoning process"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "qwen3-thinking"

    - name: "qwen3-235b"
      label: "Qwen3 235B Instruct"
      description: "Large instruct model - excellent for following instructions"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "qwen3-235b"

    - name: "minimax-m2"
      label: "Minimax M2"
      description: "Efficient general purpose model"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "minimax-m2"

    - name: "llama-3.3-70b"
      label: "Llama 3.3 70B"
      description: "Meta's latest Llama model - strong general capabilities"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "llama-3.3-70b"

    - name: "llama-4-maverick"
      label: "Llama 4 Maverick 17B"
      description: "Early preview - creative problem solving and innovation"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "llama-4-maverick"

    - name: "llama-4-scout"
      label: "Llama 4 Scout 17B"
      description: "Early preview - excellent for technical documentation and log analysis"
      group: "Vertex-AI"
      preset:
        endpoint: "Vertex-AI"
        model: "llama-4-scout"

    # Mistral Models
    - name: "mistral-large-latest"
      label: "Mistral Large"
      description: "Flagship model - best for complex reasoning"
      group: "Mistral"
      preset:
        endpoint: "Mistral"
        model: "mistral-large-latest"

    - name: "mistral-medium-latest"
      label: "Mistral Medium"
      description: "Balanced performance and cost"
      group: "Mistral"
      preset:
        endpoint: "Mistral"
        model: "mistral-medium-latest"

    - name: "mistral-small-latest"
      label: "Mistral Small"
      description: "Fast and efficient for most tasks"
      group: "Mistral"
      preset:
        endpoint: "Mistral"
        model: "mistral-small-latest"

    - name: "codestral-latest"
      label: "Codestral"
      description: "Specialized for coding tasks"
      group: "Mistral"
      preset:
        endpoint: "Mistral"
        model: "codestral-latest"

    # Perplexity Models
    - name: "sonar"
      label: "Sonar"
      description: "Lightweight search with 127k context"
      group: "Perplexity"
      preset:
        endpoint: "Perplexity"
        model: "sonar"

    - name: "sonar-pro"
      label: "Sonar Pro"
      description: "Advanced search with 200k context"
      group: "Perplexity"
      preset:
        endpoint: "Perplexity"
        model: "sonar-pro"

    - name: "sonar-reasoning"
      label: "Sonar Reasoning"
      description: "Real-time reasoning with search (127k context)"
      group: "Perplexity"
      preset:
        endpoint: "Perplexity"
        model: "sonar-reasoning"

    - name: "sonar-reasoning-pro"
      label: "Sonar Reasoning Pro"
      description: "DeepSeek-R1 powered reasoning with search (127k context)"
      group: "Perplexity"
      preset:
        endpoint: "Perplexity"
        model: "sonar-reasoning-pro"

    - name: "sonar-deep-research"
      label: "Sonar Deep Research"
      description: "Long-form research reports with citations"
      group: "Perplexity"
      preset:
        endpoint: "Perplexity"
        model: "sonar-deep-research"

interface:
  privacyPolicy:
    externalUrl: 'https://librechat.ai/privacy-policy'
    openNewTab: true
  termsOfService:
    externalUrl: 'https://librechat.ai/tos'
    openNewTab: true
  endpointsMenu: true
  modelSelect: true
  parameters: true
  sidePanel: true
  presets: true
  prompts: true
  bookmarks: true
  multiConvo: true
  agents: true
  # Agent Marketplace - enables internal agent sharing
  peoplePicker:
    users: true      # Allow selecting specific users when sharing agents
    groups: true     # Allow selecting groups
    roles: true      # Allow selecting roles (e.g., all admins)
  marketplace:
    use: true        # Enable the agent marketplace UI

registration:
  socialLogins: ['github', 'google', 'discord', 'openid']

endpoints:
  custom:
    # GCP Vertex AI Models via Custom OAuth2 Proxy
    - name: 'Vertex-AI'
      apiKey: 'dummy'  # Not used - authentication handled by vertex-proxy
      baseURL: 'http://vertex-proxy:4000'
      iconURL: '/assets/vertex-ai.svg'  

      models:
        default:
          # Reasoning models
          - 'deepseek-r1'              # DeepSeek R1 - Advanced reasoning (text-only)
          - 'qwen3-thinking'           # Qwen3 Next 80B - Thinking mode (text-only)

          # Text generation models
          - 'deepseek-v3'              # DeepSeek V3 - General purpose (text-only)
          - 'qwen3-235b'               # Qwen3 235B - Instruct (text-only)
          - 'minimax-m2'               # Minimax M2 (text-only)
          - 'llama-3.3-70b'            # Llama 3.3 70B (text-only)
          - 'llama-4-maverick'         # Llama 4 Maverick 17B (text-only)
          - 'llama-4-scout'            # Llama 4 Scout 17B (text-only)

          # Vision models (EXPERIMENTAL - test before using)
          # - 'deepseek-ocr'           # DeepSeek OCR (BROKEN - returns garbage)
          # - 'qwen3-vl-235b'          # Qwen3-VL 235B (vision+text) - NOT YET CONFIGURED

        fetch: false

      titleConvo: true
      titleModel: 'deepseek-v3'
      modelDisplayLabel: 'GCP Vertex AI'

      # Don't drop parameters - let the proxy handle them
      dropParams: []

      # Vision models configuration (DISABLED until working models confirmed)
      # Uncomment and test individual models
      # fileConfig:
      #   endpoints:
      #     - 'deepseek-ocr'         # BROKEN - returns hallucinated garbage
      #     - 'qwen3-vl-235b'        # Test this if available in your region
      #   supportedMimeTypes:
      #     - 'image/png'
      #     - 'image/jpeg'
      #     - 'image/jpg'
      #     - 'image/webp'
      #     - 'image/gif'
      #   maxFileSize: 20971520  # 20MB

  custom:
    # Mistral AI
    # Requires MISTRAL_API_KEY in .env
    - name: 'Mistral'
      apiKey: '${MISTRAL_API_KEY}'
      baseURL: 'https://api.mistral.ai/v1'

      models:
        default:
          - 'mistral-large-latest'
          - 'mistral-medium-latest'
          - 'mistral-small-latest'
          - 'codestral-latest'
        fetch: false

      titleConvo: true
      titleModel: 'mistral-small-latest'
      modelDisplayLabel: 'Mistral'

      # Mistral requires dropping these parameters to avoid 422 errors
      dropParams: ['stop', 'user', 'frequency_penalty', 'presence_penalty', 'top_p', 'logit_bias']

    # Perplexity AI
    # Requires PERPLEXITY_API_KEY in .env
    - name: 'Perplexity'
      apiKey: '${PERPLEXITY_API_KEY}'
      baseURL: 'https://api.perplexity.ai'

      models:
        default:
          - 'sonar'                    # Lightweight search (127k context)
          - 'sonar-pro'                # Advanced search (200k context)
          - 'sonar-reasoning'          # Real-time reasoning with search (127k)
          - 'sonar-reasoning-pro'      # DeepSeek-R1 powered reasoning (127k)
          - 'sonar-deep-research'      # Long-form research reports
        fetch: false

      titleConvo: true
      titleModel: 'sonar'
      modelDisplayLabel: 'Perplexity'

      # Perplexity doesn't support these OpenAI parameters
      dropParams: ['stop', 'user', 'frequency_penalty', 'presence_penalty', 'top_p']

      # Perplexity-specific parameters (optional)
      # addParams:
      #   return_citations: true
      #   return_images: false
      #   return_related_questions: false

# Web Search Configuration
# Enables web search capabilities for AI agents
webSearch:
  # Search Provider: Serper (Google Search API)
  serperApiKey: '${SERPER_API_KEY}'
  searchProvider: 'serper'

  # Content Scraper: Also using Serper (supports both search and scraping)
  scraperProvider: 'serper'
  scraperTimeout: 10000  # 10 seconds timeout

  # Reranker: Jina AI for result ranking and relevance
  jinaApiKey: '${JINA_API_KEY}'
  # jinaApiUrl: '${JINA_API_URL}'  # Optional - uncomment if using custom Jina endpoint
  rerankerType: 'jina'

  # Safe search level: 0 (off), 1 (moderate), 2 (strict)
  safeSearch: 1

# NOTE: Judge0 Code Execution is integrated directly into the Code Interpreter
# No MCP configuration needed! Just click the Code Interpreter button and enter your RapidAPI key.
# Get FREE RapidAPI key: https://rapidapi.com/judge0-official/api/judge0-ce
